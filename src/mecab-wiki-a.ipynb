{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TBThUshQOEh",
    "outputId": "dc10b791-09e9-4d3d-832f-7db94db989c3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHLA5RShQOEi",
    "outputId": "0332d88f-d81e-4378-ff9e-aefd9c9baed6"
   },
   "outputs": [],
   "source": [
    "!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n",
    "!pip install mecab-python3 pandas\n",
    "!pip install unidic\n",
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWB-AWjiQOEi",
    "outputId": "532f656b-6ed9-4074-94a9-00b9241fb970"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import html\n",
    "import json\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MeCabを使ってWikipedia本文から名詞を抽出してCSVに書き込む関数\n",
    "def extract_nouns_mecab(directory, output_csv, max_articles=None, max_nouns_per_row=10):\n",
    "    \"\"\"\n",
    "    WikipediaのJSONファイルを処理し、名詞をCSVに書き出す関数。\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): JSONファイルが格納されているディレクトリ\n",
    "    - output_csv (str): 出力CSVファイルのパス\n",
    "    - max_articles (int, optional): 処理する最大記事数\n",
    "    - max_nouns_per_row (int): 1行に書き込む名詞の最大数\n",
    "    \"\"\"\n",
    "    tagger = MeCab.Tagger()\n",
    "\n",
    "    # 既存のCSVから処理済みArticle_IDを取得\n",
    "    processed_ids = set()\n",
    "    if os.path.exists(output_csv):\n",
    "        print(\"既存のCSVファイルを読み込み中...\")\n",
    "        try:\n",
    "            df = pd.read_csv(output_csv, usecols=['Article_ID'], dtype={'Article_ID': str})\n",
    "            processed_ids = set(df['Article_ID'].unique())\n",
    "            print(f\"処理済みID数: {len(processed_ids)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"CSVファイル読み込みエラー: {e}\")\n",
    "\n",
    "    # JSONファイルの検索\n",
    "    json_files = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n",
    "    print(f\"見つかったJSONファイルの数: {len(json_files)}\")\n",
    "\n",
    "    # CSVに結果を書き込み\n",
    "    with open(output_csv, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        if os.stat(output_csv).st_size == 0:\n",
    "            csv_writer.writerow(['Article_ID', 'Title', 'Nouns'])\n",
    "\n",
    "        articles_processed = 0  # 処理した記事数のカウンタ\n",
    "\n",
    "        for json_file in tqdm(json_files, desc=\"JSONファイルの処理\"):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if max_articles and articles_processed >= max_articles:\n",
    "                            print(\"指定された記事数に達しました。処理を終了します。\")\n",
    "                            return\n",
    "\n",
    "                        article = json.loads(line)\n",
    "                        article_id = str(article.get('id', ''))\n",
    "                        if article_id in processed_ids:  # 処理済みIDはスキップ\n",
    "                            continue\n",
    "\n",
    "                        title = article.get('title', '')\n",
    "                        text = html.unescape(article.get('text', ''))\n",
    "\n",
    "                        # MeCabで形態素解析し、名詞を抽出\n",
    "                        nouns = []\n",
    "                        for line in tagger.parse(text).splitlines():\n",
    "                            if not line.strip() or line == 'EOS':  # 空行やEOSをスキップ\n",
    "                                continue\n",
    "                            parts = line.split('\\t')\n",
    "                            if len(parts) < 2:\n",
    "                                continue\n",
    "                            features = parts[1].split(',')\n",
    "                            if features[0] == '名詞':  # 名詞の判定\n",
    "                                nouns.append(parts[0])\n",
    "\n",
    "                        # 名詞リストを複数行に分割して書き込み\n",
    "                        for i in range(0, len(nouns), max_nouns_per_row):\n",
    "                            chunk = nouns[i:i + max_nouns_per_row]\n",
    "                            csv_writer.writerow([article_id, title, ', '.join(chunk)])\n",
    "\n",
    "                        processed_ids.add(article_id)\n",
    "                        articles_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ファイル {json_file} の処理中にエラーが発生しました: {e}\")\n",
    "\n",
    "    print(f\"処理が完了しました。結果は {output_csv} に保存されています。\")\n",
    "\n",
    "# 関数を実行\n",
    "input_directory = \"/content/drive/MyDrive/kirikuchikun-ai/wikipedia_json_output\"  # JSONファイルが格納されているディレクトリ\n",
    "output_csv = \"/content/drive/MyDrive/kirikuchikun-ai/csv/wikipedia-a.csv\"    # 出力CSVの保存先\n",
    "extract_nouns_mecab(input_directory, output_csv)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
