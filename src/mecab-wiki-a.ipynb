{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idutsu/kirikuchikun-ai/blob/main/src/mecab-wiki-a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TBThUshQOEh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHLA5RShQOEi"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3 pandas\n",
        "!pip install unidic\n",
        "!python -m unidic download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWB-AWjiQOEi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import glob\n",
        "import html\n",
        "import json\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MeCabを使ってWikipedia本文から名詞を抽出してCSVに書き込む関数\n",
        "def extract_nouns_mecab(directory, output_csv, max_articles=None, max_nouns_per_row=10):\n",
        "    \"\"\"\n",
        "    WikipediaのJSONファイルを処理し、名詞をCSVに書き出す関数。\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): JSONファイルが格納されているディレクトリ\n",
        "    - output_csv (str): 出力CSVファイルのパス\n",
        "    - max_articles (int, optional): 処理する最大記事数\n",
        "    - max_nouns_per_row (int): 1行に書き込む名詞の最大数\n",
        "    \"\"\"\n",
        "    tagger = MeCab.Tagger()\n",
        "\n",
        "    # 既存のCSVから処理済みArticle_IDを取得\n",
        "    processed_ids = set()\n",
        "    if os.path.exists(output_csv):\n",
        "        print(\"既存のCSVファイルを読み込み中...\")\n",
        "        try:\n",
        "            df = pd.read_csv(output_csv, usecols=['Article_ID'], dtype={'Article_ID': str})\n",
        "            processed_ids = set(df['Article_ID'].unique())\n",
        "            print(f\"処理済みID数: {len(processed_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"CSVファイル読み込みエラー: {e}\")\n",
        "\n",
        "    # JSONファイルの検索\n",
        "    json_files = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n",
        "    print(f\"見つかったJSONファイルの数: {len(json_files)}\")\n",
        "\n",
        "    # CSVに結果を書き込み\n",
        "    with open(output_csv, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        if os.stat(output_csv).st_size == 0:\n",
        "            csv_writer.writerow(['Article_ID', 'Title', 'Nouns'])\n",
        "\n",
        "        articles_processed = 0  # 処理した記事数のカウンタ\n",
        "\n",
        "        for json_file in tqdm(json_files, desc=\"JSONファイルの処理\"):\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        if max_articles and articles_processed >= max_articles:\n",
        "                            print(\"指定された記事数に達しました。処理を終了します。\")\n",
        "                            return\n",
        "\n",
        "                        article = json.loads(line)\n",
        "                        article_id = str(article.get('id', ''))\n",
        "                        if article_id in processed_ids:  # 処理済みIDはスキップ\n",
        "                            continue\n",
        "\n",
        "                        title = article.get('title', '')\n",
        "                        text = html.unescape(article.get('text', ''))\n",
        "\n",
        "                        # MeCabで形態素解析し、名詞を抽出\n",
        "                        nouns = []\n",
        "                        for line in tagger.parse(text).splitlines():\n",
        "                            if not line.strip() or line == 'EOS':  # 空行やEOSをスキップ\n",
        "                                continue\n",
        "                            parts = line.split('\\t')\n",
        "                            if len(parts) < 2:\n",
        "                                continue\n",
        "                            features = parts[1].split(',')\n",
        "                            if features[0] == '名詞':  # 名詞の判定\n",
        "                                nouns.append(parts[0])\n",
        "\n",
        "                        # 名詞リストを複数行に分割して書き込み\n",
        "                        for i in range(0, len(nouns), max_nouns_per_row):\n",
        "                            chunk = nouns[i:i + max_nouns_per_row]\n",
        "                            csv_writer.writerow([article_id, title, ', '.join(chunk)])\n",
        "\n",
        "                        processed_ids.add(article_id)\n",
        "                        articles_processed += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ファイル {json_file} の処理中にエラーが発生しました: {e}\")\n",
        "\n",
        "    print(f\"処理が完了しました。結果は {output_csv} に保存されています。\")\n",
        "\n",
        "# 関数を実行\n",
        "input_directory = \"/content/drive/MyDrive/kirikuchikun-ai/wikipedia_json_output\"  # JSONファイルが格納されているディレクトリ\n",
        "output_csv = \"/content/drive/MyDrive/kirikuchikun-ai/csv/wikipedia-a.csv\"    # 出力CSVの保存先\n",
        "extract_nouns_mecab(input_directory, output_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CSVファイルを読み込む\n",
        "df = pd.read_csv(output_csv)"
      ],
      "metadata": {
        "id": "UsNq9RBdDOWC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSVファイルの記事数をカウント\n",
        "try:\n",
        "    # Article_IDの種類をカウント\n",
        "    unique_ids = df['Article_ID'].nunique()\n",
        "    print(f\"CSVファイルに含まれるArticle_IDの種類: {unique_ids}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n"
      ],
      "metadata": {
        "id": "gQavmmb2GLoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 任意の行を抜き出す\n",
        "target_row = 1000000  # 例: 10行目を抜き出す\n",
        "\n",
        "# 任意の行を表示\n",
        "print(f\"{target_row} 行目のデータ:\")\n",
        "print(df.iloc[target_row])"
      ],
      "metadata": {
        "id": "HR-BsPp0EEZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 任意の記事の名詞を全て抜き出す\n",
        "\n",
        "target_id = 1  # 確認したいArticle_ID\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "try:\n",
        "    # 対象Article_IDの行をフィルタリング\n",
        "    filtered_rows = df[df['Article_ID'] == target_id]\n",
        "\n",
        "    # Nouns列の名詞を全て結合してリストにまとめる\n",
        "    all_nouns = []\n",
        "    for row in filtered_rows['Nouns'].dropna():  # Nouns列が空でない場合のみ処理\n",
        "        nouns_list = row.split(',')  # カンマ区切りの名詞をリストに分割\n",
        "        nouns_list = [noun.strip() for noun in nouns_list]  # 空白を取り除く\n",
        "        all_nouns.extend(nouns_list)\n",
        "\n",
        "    # 結果を表示\n",
        "    if all_nouns:\n",
        "        print(f\"Article_ID {target_id} の名詞一覧:\")\n",
        "        print(all_nouns)\n",
        "    else:\n",
        "        print(f\"Article_ID {target_id} の名詞は見つかりませんでした。\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"エラーが発生しました: {e}\")\n"
      ],
      "metadata": {
        "id": "iC-bymjNAy3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}