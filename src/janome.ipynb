{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idutsu/kirikuchikun-ai/blob/main/src/janome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UPsKVlwqQZez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリをインストール\n",
        "!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3 pandas\n",
        "!pip install unidic\n",
        "!python -m unidic download\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import glob\n",
        "import html\n",
        "import json\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MeCabを使ってwikipedia本文から「名詞/を/動詞」のパターンを抽出してCSVに書き込む関数\n",
        "def extract_noun_particle_verb_mecab(directory, output_csv, max_articles=None):\n",
        "\n",
        "    tagger = MeCab.Tagger()\n",
        "\n",
        "    # 既存のCSVから処理済みArticle_IDを取得（文字列型に統一）\n",
        "    processed_ids = set()\n",
        "    if os.path.exists(output_csv):\n",
        "        print(\"既存のCSVファイルを読み込み中...\")\n",
        "        try:\n",
        "            df = pd.read_csv(output_csv, usecols=['Article_ID'], dtype={'Article_ID': str})\n",
        "            processed_ids = set(df['Article_ID'].unique())\n",
        "            print(f\"処理済みID数: {len(processed_ids)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"CSVファイル読み込みエラー: {e}\")\n",
        "\n",
        "    # JSONファイルの検索\n",
        "    json_files = glob.glob(os.path.join(directory, '**', '*.json'), recursive=True)\n",
        "    print(f\"見つかったJSONファイルの数: {len(json_files)}\")\n",
        "\n",
        "    # CSVに結果を書き込み\n",
        "    with open(output_csv, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        if os.stat(output_csv).st_size == 0:\n",
        "            csv_writer.writerow(['Article_ID', 'Title', 'Extracted_Phrase'])\n",
        "\n",
        "        articles_processed = 0\n",
        "        for json_file in tqdm(json_files, desc=\"JSONファイルの処理\"):\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        if max_articles and articles_processed >= max_articles:\n",
        "                            print(\"指定された記事数に達しました。処理を終了します。\")\n",
        "                            return\n",
        "\n",
        "                        article = json.loads(line)\n",
        "                        article_id = str(article.get('id', ''))\n",
        "                        if article_id in processed_ids:  # 処理済みIDはスキップ\n",
        "                            continue\n",
        "\n",
        "                        title = article.get('title', '')\n",
        "                        text = html.unescape(article.get('text', ''))\n",
        "\n",
        "                        # MeCabで形態素解析\n",
        "                        parsed = tagger.parse(text)\n",
        "                        tokens = [line.split('\\t') for line in parsed.splitlines() if '\\t' in line]\n",
        "\n",
        "                        # トークンリスト作成: (表層形, 品詞1, 基本形) の取得\n",
        "                        token_list = []\n",
        "                        for line in tagger.parse(text).splitlines():\n",
        "                            if not line.strip() or line == 'EOS':  # 空行やEOSをスキップ\n",
        "                                continue\n",
        "                            parts = line.split('\\t')\n",
        "                            if len(parts) < 2:  # タブ区切りの要素が足りない場合はスキップ\n",
        "                                continue\n",
        "\n",
        "                            surface = parts[0]  # 表層形\n",
        "                            features = parts[1].split(',')\n",
        "                            pos1 = features[0]  # 品詞1\n",
        "                            base_form = features[6] if len(features) > 6 and features[6] != '*' else surface  # 基本形\n",
        "                            token_list.append((surface, pos1, base_form))\n",
        "\n",
        "                        # デバッグ: トークンの内容を確認（先頭20個）\n",
        "                        # print(\"----- トークン一覧（先頭20個）-----\")\n",
        "                        # for token in token_list[:20]:\n",
        "                        #     print(token)\n",
        "                        # print(\"------------------------\")\n",
        "\n",
        "                        # 名詞 + 助詞「を」 + 動詞 のパターンマッチング\n",
        "                        for i in range(len(token_list) - 2):\n",
        "                            first, second, third = token_list[i], token_list[i+1], token_list[i+2]\n",
        "                            if (first[1] == '名詞' and second[0] == 'を' and third[1] == '動詞'):\n",
        "                                phrase = first[0] + second[0] + third[0]  # 動詞は基本形ではなく表層形を使用\n",
        "                                #print(f\"マッチしたフレーズ: {phrase}\")  # デバッグ出力\n",
        "                                csv_writer.writerow([article_id, title, phrase])\n",
        "\n",
        "                        processed_ids.add(article_id)\n",
        "                        articles_processed += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ファイル {json_file} の処理中にエラーが発生しました: {e}\")\n",
        "\n",
        "    print(f\"処理が完了しました。結果は {output_csv} に保存されています。\")\n",
        "\n",
        "# 関数を実行\n",
        "input_directory = \"/content/drive/MyDrive/kirikuchikun-ai/wikipedia_json_output\"  # JSONファイルが格納されているディレクトリ\n",
        "output_csv = \"/content/drive/MyDrive/kirikuchikun-ai/csv/wikipedia-a-to-b.csv\"    # 出力CSVの保存先\n",
        "extract_noun_particle_verb_mecab(input_directory, output_csv)\n"
      ],
      "metadata": {
        "id": "8P9ymMcHIm05"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}